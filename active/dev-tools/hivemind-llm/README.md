# HiveMind LLM

**Distributed Browser-Based LLM Inference â€” The More Users, The Smarter It Gets**

HiveMind LLM is a chat application that runs large language models directly in users' browsers using WebGPU. What makes it unique: as more users join the chat, their combined computing power automatically unlocks access to larger, more capable models.

## ğŸ¯ Core Concept

```
1 user  â†’ TinyLlama 1.1B (runs locally)
5 users â†’ Phi-2 2.7B (distributed across browsers)
10+ users â†’ Llama 3.2 3B or larger (fully distributed)
```

Unlike traditional LLM services where the server does all the work, HiveMind distributes model inference across all connected users. Each browser handles a portion of the model's layers, coordinating via WebRTC for peer-to-peer communication.

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        Chat Interface                            â”‚
â”‚                    (React + TypeScript)                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Coordinator Service                           â”‚
â”‚  â€¢ Peer discovery & WebRTC signaling                            â”‚
â”‚  â€¢ Cluster capacity tracking                                     â”‚
â”‚  â€¢ Model selection based on available compute                   â”‚
â”‚  â€¢ Token routing between peers                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â–¼                    â–¼                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    Browser 1    â”‚  â”‚    Browser 2    â”‚  â”‚    Browser 3    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Layers   â”‚  â”‚  â”‚  â”‚  Layers   â”‚  â”‚  â”‚  â”‚  Layers   â”‚  â”‚
â”‚  â”‚   0-10    â”‚  â”‚  â”‚  â”‚  11-20    â”‚  â”‚  â”‚  â”‚  21-31    â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚    WebGPU       â”‚  â”‚    WebGPU       â”‚  â”‚    WebGPU       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                    â”‚                    â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ WebRTC P2P Mesh â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸš€ How It Works

1. **User Joins**: When you open HiveMind, your browser checks WebGPU capability and reports available VRAM to the coordinator.

2. **Cluster Formation**: The coordinator tracks all connected peers and their compute capacity.

3. **Model Selection**: Based on total cluster capacity, the coordinator selects the largest model that can run efficiently:
   - < 4GB total VRAM â†’ TinyLlama 1.1B (single peer)
   - 4-8GB â†’ Phi-2 2.7B (2-3 peers)
   - 8-16GB â†’ Llama 3.2 3B (4-6 peers)
   - 16GB+ â†’ Larger models as capacity allows

4. **Layer Distribution**: Model layers are assigned to peers based on their available VRAM. Peers download only their assigned layers.

5. **Inference Pipeline**:
   - User sends a message
   - Tokens are embedded on the first peer
   - Hidden states flow through peers in sequence (pipeline parallelism)
   - Final peer generates output tokens
   - Response streams back to the user

6. **Dynamic Scaling**: As peers join/leave, the system automatically rebalances layers and may upgrade/downgrade the active model.

## ğŸ”§ Technology Stack

### Frontend
- **React** + **TypeScript** for the chat UI
- **WebGPU** via [@aspect-build/aspect-webassets](https://github.com/aspect-build/aspect-webassets) for GPU compute
- **WebLLM** (MLC-AI) for browser-based model inference
- **WebRTC** for peer-to-peer hidden state transfer

### Backend (Coordinator)
- **Python** + **Flask** for signaling server
- **WebSocket** for real-time peer coordination
- **Redis** (optional) for multi-instance coordinator scaling

### ML/Inference
- **MLC-compiled models** in WebGPU format
- Custom layer partitioning for distributed inference
- Quantized models (4-bit) to minimize VRAM requirements

## ğŸ“¦ Project Structure

```
hivemind-llm/
â”œâ”€â”€ coordinator/           # Python signaling server
â”‚   â”œâ”€â”€ app.py            # Flask application
â”‚   â”œâ”€â”€ cluster.py        # Cluster state management
â”‚   â”œâ”€â”€ models.py         # Model registry & selection logic
â”‚   â””â”€â”€ requirements.txt
â”œâ”€â”€ frontend/             # React chat application
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ components/   # UI components
â”‚   â”‚   â”œâ”€â”€ inference/    # WebGPU inference engine
â”‚   â”‚   â”œâ”€â”€ network/      # WebRTC & signaling
â”‚   â”‚   â””â”€â”€ App.tsx
â”‚   â”œâ”€â”€ package.json
â”‚   â””â”€â”€ vite.config.ts
â”œâ”€â”€ shared/               # Shared types & protocols
â”‚   â””â”€â”€ protocol.ts       # Message types for peer communication
â””â”€â”€ docs/
    â””â”€â”€ architecture.md   # Detailed architecture documentation
```

## ğŸ® Usage

### Running Locally

```bash
# Start the coordinator
cd coordinator
pip install -r requirements.txt
python app.py

# In another terminal, start the frontend
cd frontend
npm install
npm run dev
```

### Docker

```bash
docker-compose up
```

Then open `http://localhost:5173` in multiple browser windows to see the distributed inference in action!

## ğŸ§ª Development Status

This is an experimental project exploring browser-based distributed LLM inference. Current status:

- [ ] Coordinator service (signaling + cluster management)
- [ ] WebGPU capability detection
- [ ] Single-peer inference with WebLLM
- [ ] WebRTC peer mesh
- [ ] Distributed layer execution
- [ ] Dynamic model switching
- [ ] Chat UI

## ğŸ”® Future Ideas

- **Incentive Layer**: Reward contributors with tokens or priority access
- **Persistence**: Cache model layers in IndexedDB for faster rejoins
- **Privacy Mode**: Encrypted hidden states for sensitive conversations
- **Mobile Support**: Extend to WebGPU-capable mobile browsers
- **Federated Training**: Allow the swarm to fine-tune models collaboratively

## ğŸ“š Inspiration

- [Petals](https://petals.dev/) - BitTorrent-style distributed LLM inference
- [WebLLM](https://webllm.mlc.ai/) - High-performance in-browser LLM inference
- [BOINC](https://boinc.berkeley.edu/) - Volunteer distributed computing

## ğŸ“„ License

MIT License - See [LICENSE](LICENSE) for details.
